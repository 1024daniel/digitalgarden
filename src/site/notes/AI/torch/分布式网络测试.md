---
{"dg-publish":true,"permalink":"/AI/torch/分布式网络测试/","noteIcon":"3"}
---

https://discuss.pytorch.org/t/questions-on-underlying-port-restrictions-in-nccl-gloo-communication/171573/17

In a distributed training system where you have two nodes, each with 8 GPUs, and you are using a framework like TensorFlow with NVIDIA's NCCL library for inter-GPU communication, the communication setup typically involves each process (or rank) having its own unique port for communication. This is necessary to establish individual connections between each pair of GPUs across different nodes.

Here's how the port configuration generally works in this scenario:

1. **Unique Ports per Pair of GPUs**:
    
    - Each GPU on a node communicates with a corresponding GPU on the other node. For example, GPU 0 on Node A communicates with GPU 0 on Node B, GPU 1 on Node A communicates with GPU 1 on Node B, and so on.
    - To facilitate this communication, each pair of GPUs (across nodes) will use a specific port number for NCCL communication. These port numbers are typically unique for each pair.
2. **Port Configuration**:
    
    - When launching your distributed training job, you would specify the port numbers to be used for NCCL communication.
    - Each process (or rank) participating in the training job will be configured with its own NCCL port number. For example, if you have 8 processes (one for each GPU) on Node A and 8 processes on Node B, each process will have a unique NCCL port configured.
3. **NCCL Communication**:
    
    - NCCL library handles the communication between GPUs efficiently using these configured port numbers.
    - The NCCL library ensures that each process can establish connections with the corresponding GPU processes on other nodes using the specified port numbers.

#allreduce
```py
import torch
import torch.distributed as dist

def main():
    # Initialize the process group
    dist.init_process_group(backend='gloo', init_method='tcp://<IP_of_Node_1>:<Port>', rank=0, world_size=2)

    # Input tensor on Node 1
    tensor = torch.tensor([1.0, 2.0, 3.0])
    
    # 执行张量广播操作
    #dist.broadcast(tensor, src=0)  # 将排名为0的节点的张量广播到所有其他节点
    # 打印广播后的张量
    #print(f"Rank {rank} - Broadcasted Tensor: {tensor}")

    # All-reduce operation
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)

    # Output the reduced tensor
    print(f"Node 1 - Reduced Tensor: {tensor}")

    # Clean up
    dist.destroy_process_group()

if __name__ == '__main__':
    main()


```