---
{"dg-publish":true,"permalink":"/AI/torch/分布式网络测试/","noteIcon":"3"}
---

https://discuss.pytorch.org/t/questions-on-underlying-port-restrictions-in-nccl-gloo-communication/171573/17

#allreduce
```py
import torch
import torch.distributed as dist

def main():
    # Initialize the process group
    dist.init_process_group(backend='gloo', init_method='tcp://<IP_of_Node_1>:<Port>', rank=0, world_size=2)

    # Input tensor on Node 1
    tensor = torch.tensor([1.0, 2.0, 3.0])
    
    # 执行张量广播操作
    #dist.broadcast(tensor, src=0)  # 将排名为0的节点的张量广播到所有其他节点
    # 打印广播后的张量
    #print(f"Rank {rank} - Broadcasted Tensor: {tensor}")

    # All-reduce operation
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)

    # Output the reduced tensor
    print(f"Node 1 - Reduced Tensor: {tensor}")

    # Clean up
    dist.destroy_process_group()

if __name__ == '__main__':
    main()


```