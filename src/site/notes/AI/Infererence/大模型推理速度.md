---
{"dg-publish":true,"permalink":"/AI/Infererence/大模型推理速度/","noteIcon":"3"}
---

![jessica-felicio-BzSfM6n_LIE-unsplash.jpg](/img/user/banner/jessica-felicio-BzSfM6n_LIE-unsplash.jpg)

#inference 

https://blog.csdn.net/qq_25295605/article/details/141140475

https://pdf.dfcfw.com/pdf/H3_AP202308241595683642_1.pdf?1692865920000.pdf

大模型推理速度一般有多个统计维度：
- 首token延迟，TTFT(Time to First Token), prefill, Prefing指的都是从输入到输出第一个token的延迟, (第一个token生成没有kv缓存，需要生成，耗时较长，后面token有缓存kv，空间换时间)
- TPOP(Time Per Output Token)， 不包含首token的每个输出token的延迟，指的是第2个token开始吐出的速度
- 延迟Lantency， Latency= TTFT + (TPOP)* totoal number of tokens to be generated
- Tokens Per Second(TPS)  (the number of tokens to be generated)/Latency


大模型推理计算量:
![Pasted image 20241010003636.png](/img/user/AI/Infererence/attachments/Pasted%20image%2020241010003636.png)

在推理时，每个 token 都需要经过模型的每个参数进行一次计算。

每个计算单位涉及一次乘法和一次加法，因此总计算量为每个 token 乘以每个参数再乘以 2。

因此评估大模型推理速度的一般公式可以看做:
Perak Flops of GPU * Flops Utilization / (2 * ModelParameter * Prompt Tokens)

对于Llama-3.1 70B 模型，prompt为1k tokens时，10张A100(312 FP16 TFlops) ,假定利用率为46.2%
平均速度为:`10 * 312T * 0.462 / (2 * 70B * 1k) = 10.3 tokens/s`

